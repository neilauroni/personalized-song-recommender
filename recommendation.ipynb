{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1733171939341,
     "user": {
      "displayName": "Neil Auroni",
      "userId": "12234025139407697306"
     },
     "user_tz": 300
    },
    "id": "AO40SyEPGCp6"
   },
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iJMVRNMXGggP"
   },
   "source": [
    "### Audio Preprocessing ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1733171891248,
     "user": {
      "displayName": "Neil Auroni",
      "userId": "12234025139407697306"
     },
     "user_tz": 300
    },
    "id": "Qd4JLyq1FTbv"
   },
   "outputs": [],
   "source": [
    "def preprocess_audio(file_path, duration=30, sr=16000):\n",
    "    y, _ = librosa.load(file_path, sr=sr)\n",
    "    if len(y) > sr * duration:\n",
    "        y = y[:sr * duration]\n",
    "    else:\n",
    "        y = np.pad(y, (0, sr * duration - len(y)), 'constant')\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "make sure you have 30 second snippets of your songs in AUDIO_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 815,
     "status": "ok",
     "timestamp": 1733171923257,
     "user": {
      "displayName": "Neil Auroni",
      "userId": "12234025139407697306"
     },
     "user_tz": 300
    },
    "id": "YqO1puu_GCGC"
   },
   "outputs": [],
   "source": [
    "AUDIO_DIR = \"/content/wav_snippets\"\n",
    "PROCESSED_AUDIO_DIR = \"/content/processed_audio\"\n",
    "os.makedirs(PROCESSED_AUDIO_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 17595,
     "status": "ok",
     "timestamp": 1733171960472,
     "user": {
      "displayName": "Neil Auroni",
      "userId": "12234025139407697306"
     },
     "user_tz": 300
    },
    "id": "1OArqnB9GMTL"
   },
   "outputs": [],
   "source": [
    "audio_files = [f for f in os.listdir(AUDIO_DIR) if f.endswith(('.mp3', '.wav'))]\n",
    "\n",
    "for file_name in audio_files:\n",
    "    file_path = os.path.join(AUDIO_DIR, file_name)\n",
    "    y = preprocess_audio(file_path)\n",
    "    np.save(os.path.join(PROCESSED_AUDIO_DIR, file_name + '.npy'), y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bCZO83TTGilI"
   },
   "source": [
    "### Original Embeddings ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 5749,
     "status": "ok",
     "timestamp": 1733172050877,
     "user": {
      "displayName": "Neil Auroni",
      "userId": "12234025139407697306"
     },
     "user_tz": 300
    },
    "id": "QlIlRFGsGRRn"
   },
   "outputs": [],
   "source": [
    "from torchvggish import vggish, vggish_input\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14641,
     "status": "ok",
     "timestamp": 1733172068343,
     "user": {
      "displayName": "Neil Auroni",
      "userId": "12234025139407697306"
     },
     "user_tz": 300
    },
    "id": "lGDhuP9BGlRo",
    "outputId": "e598e312-c0da-4460-b6fa-af4b303bafe4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/harritaylor/torchvggish/releases/download/v0.1/vggish_pca_params-970ea276.pth\" to /root/.cache/torch/hub/checkpoints/vggish_pca_params-970ea276.pth\n",
      "100%|██████████| 177k/177k [00:00<00:00, 1.21MB/s]\n",
      "Downloading: \"https://github.com/harritaylor/torchvggish/releases/download/v0.1/vggish-10086976.pth\" to /root/.cache/torch/hub/checkpoints/vggish-10086976.pth\n",
      "100%|██████████| 275M/275M [00:10<00:00, 26.9MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (11): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (12): ReLU(inplace=True)\n",
       "    (13): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (14): ReLU(inplace=True)\n",
       "    (15): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (embeddings): Sequential(\n",
       "    (0): Linear(in_features=12288, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): Linear(in_features=4096, out_features=128, bias=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\") \n",
    "elif torch.cuda.is_availble:\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "model = vggish()\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 594,
     "status": "ok",
     "timestamp": 1733172079787,
     "user": {
      "displayName": "Neil Auroni",
      "userId": "12234025139407697306"
     },
     "user_tz": 300
    },
    "id": "l8Gr6xlSGsRB"
   },
   "outputs": [],
   "source": [
    "def extract_embedding(audio, sr=16000):\n",
    "    examples = vggish_input.waveform_to_examples(audio, sr)\n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.pproc._pca_matrix = model.pproc._pca_matrix.to(device)\n",
    "    model.pproc._pca_means = model.pproc._pca_means.to(device)\n",
    "    examples = examples.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        embedding = model(examples)\n",
    "\n",
    "    return embedding.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 264,
     "status": "ok",
     "timestamp": 1733172087052,
     "user": {
      "displayName": "Neil Auroni",
      "userId": "12234025139407697306"
     },
     "user_tz": 300
    },
    "id": "2sfcEVYAGyht"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 34411,
     "status": "ok",
     "timestamp": 1733172133764,
     "user": {
      "displayName": "Neil Auroni",
      "userId": "12234025139407697306"
     },
     "user_tz": 300
    },
    "id": "_3v0uCMvG0Zy",
    "outputId": "ef044ca9-a315-447f-bfac-148159279a5b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:34<00:00,  1.14s/it]\n"
     ]
    }
   ],
   "source": [
    "EMBEDDINGS_DIR = '/content/embeddings'\n",
    "os.makedirs(EMBEDDINGS_DIR, exist_ok=True)\n",
    "\n",
    "for file_name in tqdm(audio_files):\n",
    "    audio_path = os.path.join(PROCESSED_AUDIO_DIR, file_name + '.npy')\n",
    "    audio = np.load(audio_path)\n",
    "    embedding = extract_embedding(audio)\n",
    "    np.save(os.path.join(EMBEDDINGS_DIR, file_name + '_embedding.npy'), embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yaj3ypcSHm43"
   },
   "source": [
    "### Training with Feedback ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "executionInfo": {
     "elapsed": 359,
     "status": "ok",
     "timestamp": 1733172303078,
     "user": {
      "displayName": "Neil Auroni",
      "userId": "12234025139407697306"
     },
     "user_tz": 300
    },
    "id": "mRGcuM_bHmW3"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 735,
     "status": "ok",
     "timestamp": 1733172223120,
     "user": {
      "displayName": "Neil Auroni",
      "userId": "12234025139407697306"
     },
     "user_tz": 300
    },
    "id": "SJH5z6MIHAjJ"
   },
   "outputs": [],
   "source": [
    "def prepare_feedback_pairs(feedback):\n",
    "    pairs = []\n",
    "    for item in feedback:\n",
    "        try:\n",
    "            song_a = item['song_a']\n",
    "            song_b = item['song_b']\n",
    "            score = item['similarity_score']\n",
    "\n",
    "            embedding_a = np.load(os.path.join(EMBEDDINGS_DIR, song_a + '_embedding.npy'))\n",
    "            embedding_b = np.load(os.path.join(EMBEDDINGS_DIR, song_b + '_embedding.npy'))\n",
    "\n",
    "            assert embedding_a.shape == (31, 128), f\"Unexpected shape for {song_a}: {embedding_a.shape}\"\n",
    "            assert embedding_b.shape == (31, 128), f\"Unexpected shape for {song_b}: {embedding_b.shape}\"\n",
    "\n",
    "            pairs.append((embedding_a, embedding_b, score))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing pair {song_a} - {song_b}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "executionInfo": {
     "elapsed": 254,
     "status": "ok",
     "timestamp": 1733172305165,
     "user": {
      "displayName": "Neil Auroni",
      "userId": "12234025139407697306"
     },
     "user_tz": 300
    },
    "id": "04HTFlBeHVhd"
   },
   "outputs": [],
   "source": [
    "class FeedbackDataset(Dataset):\n",
    "    def __init__(self, pairs):\n",
    "        self.pairs = pairs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        embedding_a, embedding_b, score = self.pairs[idx]\n",
    "\n",
    "        embedding_a = torch.tensor(embedding_a, dtype=torch.float32)  # [31, 128]\n",
    "        embedding_b = torch.tensor(embedding_b, dtype=torch.float32)  # [31, 128]\n",
    "        score = torch.tensor(score, dtype=torch.float32)\n",
    "\n",
    "        return embedding_a, embedding_b, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "executionInfo": {
     "elapsed": 252,
     "status": "ok",
     "timestamp": 1733172321166,
     "user": {
      "displayName": "Neil Auroni",
      "userId": "12234025139407697306"
     },
     "user_tz": 300
    },
    "id": "hRgP8VY_Hiwb"
   },
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    embeddings_a, embeddings_b, scores = zip(*batch)\n",
    "\n",
    "    embeddings_a = torch.stack(embeddings_a)  # [batch_size, 31, 128]\n",
    "    embeddings_b = torch.stack(embeddings_b)  # [batch_size, 31, 128]\n",
    "    scores = torch.stack(scores)  # [batch_size]\n",
    "\n",
    "    return embeddings_a, embeddings_b, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1733172346123,
     "user": {
      "displayName": "Neil Auroni",
      "userId": "12234025139407697306"
     },
     "user_tz": 300
    },
    "id": "3s9s3Wb8Hsuf"
   },
   "outputs": [],
   "source": [
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "\n",
    "        self.seq_length = 31\n",
    "        self.embedding_dim = 128\n",
    "        self.hidden_dim = 256\n",
    "\n",
    "        self.frame_encoder = nn.Sequential(\n",
    "            nn.Linear(self.embedding_dim, self.hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(self.hidden_dim, self.embedding_dim),\n",
    "            nn.LayerNorm(self.embedding_dim)\n",
    "        )\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=self.embedding_dim,\n",
    "            nhead=8,\n",
    "            dim_feedforward=512,\n",
    "            dropout=0.2,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            encoder_layer,\n",
    "            num_layers=3\n",
    "        )\n",
    "\n",
    "        self.similarity_net = nn.Sequential(\n",
    "            nn.Linear(self.embedding_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(32, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward_one(self, x):\n",
    "        encoded = self.frame_encoder(x)  # [batch_size, 31, 128]\n",
    "        transformed = self.transformer(encoded)  # [batch_size, 31, 128]\n",
    "        pooled = torch.mean(transformed, dim=1)  # [batch_size, 128]\n",
    "\n",
    "        return pooled\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        out1 = self.forward_one(x1)  # [batch_size, 128]\n",
    "        out2 = self.forward_one(x2)  # [batch_size, 128]\n",
    "\n",
    "        diff = torch.abs(out1 - out2)  # [batch_size, 128]\n",
    "\n",
    "        similarity = self.similarity_net(diff)\n",
    "        return similarity.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "executionInfo": {
     "elapsed": 300,
     "status": "ok",
     "timestamp": 1733172364463,
     "user": {
      "displayName": "Neil Auroni",
      "userId": "12234025139407697306"
     },
     "user_tz": 300
    },
    "id": "HqmYO63JHzfF"
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    num_batches = len(dataloader)\n",
    "\n",
    "    for batch_idx, (embedding_a, embedding_b, scores) in enumerate(dataloader):\n",
    "        embedding_a = embedding_a.to(device)  # [batch_size, 31, 128]\n",
    "        embedding_b = embedding_b.to(device)  # [batch_size, 31, 128]\n",
    "        scores = scores.to(device)  # [batch_size]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(embedding_a, embedding_b)\n",
    "\n",
    "        loss = criterion(predictions, scores)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if batch_idx % 5 == 0:\n",
    "            print(f'Batch {batch_idx}/{num_batches}, Loss: {loss.item():.4f}')\n",
    "\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "executionInfo": {
     "elapsed": 262,
     "status": "ok",
     "timestamp": 1733173672248,
     "user": {
      "displayName": "Neil Auroni",
      "userId": "12234025139407697306"
     },
     "user_tz": 300
    },
    "id": "p0mBokFCH4JB"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = SiameseNetwork().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1733172457207,
     "user": {
      "displayName": "Neil Auroni",
      "userId": "12234025139407697306"
     },
     "user_tz": 300
    },
    "id": "01nsGleEIOTO"
   },
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add file_path to similarity scores json below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 247,
     "status": "ok",
     "timestamp": 1733173658605,
     "user": {
      "displayName": "Neil Auroni",
      "userId": "12234025139407697306"
     },
     "user_tz": 300
    },
    "id": "IGj66aq5II7S"
   },
   "outputs": [],
   "source": [
    "file_path = \"...\"\n",
    "\n",
    "with open(file_path, \"r\") as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1733173660531,
     "user": {
      "displayName": "Neil Auroni",
      "userId": "12234025139407697306"
     },
     "user_tz": 300
    },
    "id": "Cdh9M9Q7H5wM"
   },
   "outputs": [],
   "source": [
    "feedback_pairs = prepare_feedback_pairs(data)\n",
    "feedback_dataset = FeedbackDataset(feedback_pairs)\n",
    "feedback_dataloader = DataLoader(\n",
    "    feedback_dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 921,
     "status": "ok",
     "timestamp": 1733173662785,
     "user": {
      "displayName": "Neil Auroni",
      "userId": "12234025139407697306"
     },
     "user_tz": 300
    },
    "id": "do1Twl6tIXNa",
    "outputId": "5de09912-8f60-4061-c63a-515879441f63"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode='min',\n",
    "    factor=0.5,\n",
    "    patience=5,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1733173663204,
     "user": {
      "displayName": "Neil Auroni",
      "userId": "12234025139407697306"
     },
     "user_tz": 300
    },
    "id": "DF4zLfuEIZIp"
   },
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "best_loss = float('inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 42841,
     "status": "ok",
     "timestamp": 1733173716815,
     "user": {
      "displayName": "Neil Auroni",
      "userId": "12234025139407697306"
     },
     "user_tz": 300
    },
    "id": "uHsxhwbtIabL",
    "outputId": "913812b9-d30e-4350-ec66-953fc9639e34"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0/23, Loss: 0.0785\n",
      "Batch 5/23, Loss: 0.1235\n",
      "Batch 10/23, Loss: 0.0805\n",
      "Batch 15/23, Loss: 0.1026\n",
      "Batch 20/23, Loss: 0.0882\n",
      "Epoch 1/100, Loss: 0.0899\n",
      "Batch 0/23, Loss: 0.1095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:608: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 5/23, Loss: 0.0498\n",
      "Batch 10/23, Loss: 0.0483\n",
      "Batch 15/23, Loss: 0.0857\n",
      "Batch 20/23, Loss: 0.0763\n",
      "Epoch 2/100, Loss: 0.0807\n",
      "Batch 0/23, Loss: 0.0350\n",
      "Batch 5/23, Loss: 0.0393\n",
      "Batch 10/23, Loss: 0.0942\n",
      "Batch 15/23, Loss: 0.1017\n",
      "Batch 20/23, Loss: 0.0545\n",
      "Epoch 3/100, Loss: 0.0691\n",
      "Batch 0/23, Loss: 0.0669\n",
      "Batch 5/23, Loss: 0.0534\n",
      "Batch 10/23, Loss: 0.0618\n",
      "Batch 15/23, Loss: 0.0716\n",
      "Batch 20/23, Loss: 0.0287\n",
      "Epoch 4/100, Loss: 0.0602\n",
      "Batch 0/23, Loss: 0.1010\n",
      "Batch 5/23, Loss: 0.0706\n",
      "Batch 10/23, Loss: 0.0602\n",
      "Batch 15/23, Loss: 0.0525\n",
      "Batch 20/23, Loss: 0.0577\n",
      "Epoch 5/100, Loss: 0.0627\n",
      "Batch 0/23, Loss: 0.0804\n",
      "Batch 5/23, Loss: 0.0914\n",
      "Batch 10/23, Loss: 0.0553\n",
      "Batch 15/23, Loss: 0.0726\n",
      "Batch 20/23, Loss: 0.0884\n",
      "Epoch 6/100, Loss: 0.0552\n",
      "Batch 0/23, Loss: 0.0302\n",
      "Batch 5/23, Loss: 0.0329\n",
      "Batch 10/23, Loss: 0.0436\n",
      "Batch 15/23, Loss: 0.1198\n",
      "Batch 20/23, Loss: 0.0417\n",
      "Epoch 7/100, Loss: 0.0541\n",
      "Batch 0/23, Loss: 0.0470\n",
      "Batch 5/23, Loss: 0.0778\n",
      "Batch 10/23, Loss: 0.0386\n",
      "Batch 15/23, Loss: 0.0370\n",
      "Batch 20/23, Loss: 0.0196\n",
      "Epoch 8/100, Loss: 0.0551\n",
      "Batch 0/23, Loss: 0.0519\n",
      "Batch 5/23, Loss: 0.0430\n",
      "Batch 10/23, Loss: 0.0262\n",
      "Batch 15/23, Loss: 0.0432\n",
      "Batch 20/23, Loss: 0.0533\n",
      "Epoch 9/100, Loss: 0.0521\n",
      "Batch 0/23, Loss: 0.0429\n",
      "Batch 5/23, Loss: 0.0589\n",
      "Batch 10/23, Loss: 0.0375\n",
      "Batch 15/23, Loss: 0.0337\n",
      "Batch 20/23, Loss: 0.0305\n",
      "Epoch 10/100, Loss: 0.0560\n",
      "Batch 0/23, Loss: 0.0478\n",
      "Batch 5/23, Loss: 0.0431\n",
      "Batch 10/23, Loss: 0.0630\n",
      "Batch 15/23, Loss: 0.0423\n",
      "Batch 20/23, Loss: 0.0154\n",
      "Epoch 11/100, Loss: 0.0540\n",
      "Batch 0/23, Loss: 0.0489\n",
      "Batch 5/23, Loss: 0.0393\n",
      "Batch 10/23, Loss: 0.0526\n",
      "Batch 15/23, Loss: 0.0623\n",
      "Batch 20/23, Loss: 0.0928\n",
      "Epoch 12/100, Loss: 0.0498\n",
      "Batch 0/23, Loss: 0.0405\n",
      "Batch 5/23, Loss: 0.0455\n",
      "Batch 10/23, Loss: 0.0303\n",
      "Batch 15/23, Loss: 0.0317\n",
      "Batch 20/23, Loss: 0.0313\n",
      "Epoch 13/100, Loss: 0.0468\n",
      "Batch 0/23, Loss: 0.0594\n",
      "Batch 5/23, Loss: 0.0523\n",
      "Batch 10/23, Loss: 0.0774\n",
      "Batch 15/23, Loss: 0.0634\n",
      "Batch 20/23, Loss: 0.0486\n",
      "Epoch 14/100, Loss: 0.0495\n",
      "Batch 0/23, Loss: 0.0175\n",
      "Batch 5/23, Loss: 0.0205\n",
      "Batch 10/23, Loss: 0.0363\n",
      "Batch 15/23, Loss: 0.0423\n",
      "Batch 20/23, Loss: 0.0652\n",
      "Epoch 15/100, Loss: 0.0496\n",
      "Batch 0/23, Loss: 0.0476\n",
      "Batch 5/23, Loss: 0.0475\n",
      "Batch 10/23, Loss: 0.0378\n",
      "Batch 15/23, Loss: 0.0624\n",
      "Batch 20/23, Loss: 0.0391\n",
      "Epoch 16/100, Loss: 0.0452\n",
      "Batch 0/23, Loss: 0.0594\n",
      "Batch 5/23, Loss: 0.0315\n",
      "Batch 10/23, Loss: 0.0330\n",
      "Batch 15/23, Loss: 0.0584\n",
      "Batch 20/23, Loss: 0.0724\n",
      "Epoch 17/100, Loss: 0.0435\n",
      "Batch 0/23, Loss: 0.0301\n",
      "Batch 5/23, Loss: 0.0277\n",
      "Batch 10/23, Loss: 0.0549\n",
      "Batch 15/23, Loss: 0.0513\n",
      "Batch 20/23, Loss: 0.0385\n",
      "Epoch 18/100, Loss: 0.0437\n",
      "Batch 0/23, Loss: 0.0528\n",
      "Batch 5/23, Loss: 0.0322\n",
      "Batch 10/23, Loss: 0.0249\n",
      "Batch 15/23, Loss: 0.0202\n",
      "Batch 20/23, Loss: 0.0158\n",
      "Epoch 19/100, Loss: 0.0374\n",
      "Batch 0/23, Loss: 0.0337\n",
      "Batch 5/23, Loss: 0.0468\n",
      "Batch 10/23, Loss: 0.0322\n",
      "Batch 15/23, Loss: 0.0434\n",
      "Batch 20/23, Loss: 0.0289\n",
      "Epoch 20/100, Loss: 0.0348\n",
      "Batch 0/23, Loss: 0.0401\n",
      "Batch 5/23, Loss: 0.0428\n",
      "Batch 10/23, Loss: 0.0102\n",
      "Batch 15/23, Loss: 0.0262\n",
      "Batch 20/23, Loss: 0.0377\n",
      "Epoch 21/100, Loss: 0.0296\n",
      "Batch 0/23, Loss: 0.0179\n",
      "Batch 5/23, Loss: 0.0514\n",
      "Batch 10/23, Loss: 0.0374\n",
      "Batch 15/23, Loss: 0.0327\n",
      "Batch 20/23, Loss: 0.0208\n",
      "Epoch 22/100, Loss: 0.0301\n",
      "Batch 0/23, Loss: 0.0262\n",
      "Batch 5/23, Loss: 0.0720\n",
      "Batch 10/23, Loss: 0.0320\n",
      "Batch 15/23, Loss: 0.0364\n",
      "Batch 20/23, Loss: 0.0145\n",
      "Epoch 23/100, Loss: 0.0312\n",
      "Batch 0/23, Loss: 0.0155\n",
      "Batch 5/23, Loss: 0.0163\n",
      "Batch 10/23, Loss: 0.0183\n",
      "Batch 15/23, Loss: 0.0352\n",
      "Batch 20/23, Loss: 0.0306\n",
      "Epoch 24/100, Loss: 0.0260\n",
      "Batch 0/23, Loss: 0.0135\n",
      "Batch 5/23, Loss: 0.0248\n",
      "Batch 10/23, Loss: 0.0267\n",
      "Batch 15/23, Loss: 0.0412\n",
      "Batch 20/23, Loss: 0.0263\n",
      "Epoch 25/100, Loss: 0.0253\n",
      "Batch 0/23, Loss: 0.0315\n",
      "Batch 5/23, Loss: 0.0234\n",
      "Batch 10/23, Loss: 0.0119\n",
      "Batch 15/23, Loss: 0.0173\n",
      "Batch 20/23, Loss: 0.0281\n",
      "Epoch 26/100, Loss: 0.0211\n",
      "Batch 0/23, Loss: 0.0340\n",
      "Batch 5/23, Loss: 0.0238\n",
      "Batch 10/23, Loss: 0.0147\n",
      "Batch 15/23, Loss: 0.0066\n",
      "Batch 20/23, Loss: 0.0285\n",
      "Epoch 27/100, Loss: 0.0164\n",
      "Batch 0/23, Loss: 0.0205\n",
      "Batch 5/23, Loss: 0.0281\n",
      "Batch 10/23, Loss: 0.0113\n",
      "Batch 15/23, Loss: 0.0071\n",
      "Batch 20/23, Loss: 0.0173\n",
      "Epoch 28/100, Loss: 0.0174\n",
      "Batch 0/23, Loss: 0.0152\n",
      "Batch 5/23, Loss: 0.0218\n",
      "Batch 10/23, Loss: 0.0147\n",
      "Batch 15/23, Loss: 0.0303\n",
      "Batch 20/23, Loss: 0.0162\n",
      "Epoch 29/100, Loss: 0.0158\n",
      "Batch 0/23, Loss: 0.0126\n",
      "Batch 5/23, Loss: 0.0174\n",
      "Batch 10/23, Loss: 0.0114\n",
      "Batch 15/23, Loss: 0.0105\n",
      "Batch 20/23, Loss: 0.0185\n",
      "Epoch 30/100, Loss: 0.0175\n",
      "Batch 0/23, Loss: 0.0081\n",
      "Batch 5/23, Loss: 0.0233\n",
      "Batch 10/23, Loss: 0.0238\n",
      "Batch 15/23, Loss: 0.0287\n",
      "Batch 20/23, Loss: 0.0049\n",
      "Epoch 31/100, Loss: 0.0160\n",
      "Batch 0/23, Loss: 0.0235\n",
      "Batch 5/23, Loss: 0.0045\n",
      "Batch 10/23, Loss: 0.0086\n",
      "Batch 15/23, Loss: 0.0124\n",
      "Batch 20/23, Loss: 0.0142\n",
      "Epoch 32/100, Loss: 0.0138\n",
      "Batch 0/23, Loss: 0.0116\n",
      "Batch 5/23, Loss: 0.0113\n",
      "Batch 10/23, Loss: 0.0063\n",
      "Batch 15/23, Loss: 0.0291\n",
      "Batch 20/23, Loss: 0.0167\n",
      "Epoch 33/100, Loss: 0.0131\n",
      "Batch 0/23, Loss: 0.0180\n",
      "Batch 5/23, Loss: 0.0121\n",
      "Batch 10/23, Loss: 0.0079\n",
      "Batch 15/23, Loss: 0.0156\n",
      "Batch 20/23, Loss: 0.0135\n",
      "Epoch 34/100, Loss: 0.0156\n",
      "Batch 0/23, Loss: 0.0058\n",
      "Batch 5/23, Loss: 0.0062\n",
      "Batch 10/23, Loss: 0.0104\n",
      "Batch 15/23, Loss: 0.0144\n",
      "Batch 20/23, Loss: 0.0060\n",
      "Epoch 35/100, Loss: 0.0100\n",
      "Batch 0/23, Loss: 0.0085\n",
      "Batch 5/23, Loss: 0.0421\n",
      "Batch 10/23, Loss: 0.0068\n",
      "Batch 15/23, Loss: 0.0124\n",
      "Batch 20/23, Loss: 0.0133\n",
      "Epoch 36/100, Loss: 0.0135\n",
      "Batch 0/23, Loss: 0.0069\n",
      "Batch 5/23, Loss: 0.0074\n",
      "Batch 10/23, Loss: 0.0044\n",
      "Batch 15/23, Loss: 0.0088\n",
      "Batch 20/23, Loss: 0.0119\n",
      "Epoch 37/100, Loss: 0.0115\n",
      "Batch 0/23, Loss: 0.0068\n",
      "Batch 5/23, Loss: 0.0083\n",
      "Batch 10/23, Loss: 0.0119\n",
      "Batch 15/23, Loss: 0.0110\n",
      "Batch 20/23, Loss: 0.0107\n",
      "Epoch 38/100, Loss: 0.0115\n",
      "Batch 0/23, Loss: 0.0106\n",
      "Batch 5/23, Loss: 0.0154\n",
      "Batch 10/23, Loss: 0.0097\n",
      "Batch 15/23, Loss: 0.0086\n",
      "Batch 20/23, Loss: 0.0073\n",
      "Epoch 39/100, Loss: 0.0087\n",
      "Batch 0/23, Loss: 0.0049\n",
      "Batch 5/23, Loss: 0.0143\n",
      "Batch 10/23, Loss: 0.0124\n",
      "Batch 15/23, Loss: 0.0108\n",
      "Batch 20/23, Loss: 0.0125\n",
      "Epoch 40/100, Loss: 0.0110\n",
      "Batch 0/23, Loss: 0.0064\n",
      "Batch 5/23, Loss: 0.0110\n",
      "Batch 10/23, Loss: 0.0088\n",
      "Batch 15/23, Loss: 0.0067\n",
      "Batch 20/23, Loss: 0.0073\n",
      "Epoch 41/100, Loss: 0.0097\n",
      "Batch 0/23, Loss: 0.0181\n",
      "Batch 5/23, Loss: 0.0160\n",
      "Batch 10/23, Loss: 0.0096\n",
      "Batch 15/23, Loss: 0.0102\n",
      "Batch 20/23, Loss: 0.0061\n",
      "Epoch 42/100, Loss: 0.0118\n",
      "Batch 0/23, Loss: 0.0152\n",
      "Batch 5/23, Loss: 0.0048\n",
      "Batch 10/23, Loss: 0.0070\n",
      "Batch 15/23, Loss: 0.0059\n",
      "Batch 20/23, Loss: 0.0201\n",
      "Epoch 43/100, Loss: 0.0095\n",
      "Batch 0/23, Loss: 0.0210\n",
      "Batch 5/23, Loss: 0.0189\n",
      "Batch 10/23, Loss: 0.0177\n",
      "Batch 15/23, Loss: 0.0060\n",
      "Batch 20/23, Loss: 0.0093\n",
      "Epoch 44/100, Loss: 0.0117\n",
      "Batch 0/23, Loss: 0.0092\n",
      "Batch 5/23, Loss: 0.0078\n",
      "Batch 10/23, Loss: 0.0089\n",
      "Batch 15/23, Loss: 0.0210\n",
      "Batch 20/23, Loss: 0.0340\n",
      "Epoch 45/100, Loss: 0.0108\n",
      "Batch 0/23, Loss: 0.0037\n",
      "Batch 5/23, Loss: 0.0083\n",
      "Batch 10/23, Loss: 0.0074\n",
      "Batch 15/23, Loss: 0.0032\n",
      "Batch 20/23, Loss: 0.0090\n",
      "Epoch 46/100, Loss: 0.0091\n",
      "Batch 0/23, Loss: 0.0069\n",
      "Batch 5/23, Loss: 0.0087\n",
      "Batch 10/23, Loss: 0.0069\n",
      "Batch 15/23, Loss: 0.0253\n",
      "Batch 20/23, Loss: 0.0059\n",
      "Epoch 47/100, Loss: 0.0099\n",
      "Batch 0/23, Loss: 0.0058\n",
      "Batch 5/23, Loss: 0.0065\n",
      "Batch 10/23, Loss: 0.0075\n",
      "Batch 15/23, Loss: 0.0106\n",
      "Batch 20/23, Loss: 0.0124\n",
      "Epoch 48/100, Loss: 0.0085\n",
      "Batch 0/23, Loss: 0.0036\n",
      "Batch 5/23, Loss: 0.0096\n",
      "Batch 10/23, Loss: 0.0097\n",
      "Batch 15/23, Loss: 0.0111\n",
      "Batch 20/23, Loss: 0.0034\n",
      "Epoch 49/100, Loss: 0.0081\n",
      "Batch 0/23, Loss: 0.0107\n",
      "Batch 5/23, Loss: 0.0080\n",
      "Batch 10/23, Loss: 0.0136\n",
      "Batch 15/23, Loss: 0.0119\n",
      "Batch 20/23, Loss: 0.0051\n",
      "Epoch 50/100, Loss: 0.0097\n",
      "Batch 0/23, Loss: 0.0096\n",
      "Batch 5/23, Loss: 0.0023\n",
      "Batch 10/23, Loss: 0.0104\n",
      "Batch 15/23, Loss: 0.0168\n",
      "Batch 20/23, Loss: 0.0137\n",
      "Epoch 51/100, Loss: 0.0101\n",
      "Batch 0/23, Loss: 0.0087\n",
      "Batch 5/23, Loss: 0.0103\n",
      "Batch 10/23, Loss: 0.0049\n",
      "Batch 15/23, Loss: 0.0061\n",
      "Batch 20/23, Loss: 0.0326\n",
      "Epoch 52/100, Loss: 0.0081\n",
      "Batch 0/23, Loss: 0.0238\n",
      "Batch 5/23, Loss: 0.0022\n",
      "Batch 10/23, Loss: 0.0056\n",
      "Batch 15/23, Loss: 0.0112\n",
      "Batch 20/23, Loss: 0.0126\n",
      "Epoch 53/100, Loss: 0.0096\n",
      "Batch 0/23, Loss: 0.0057\n",
      "Batch 5/23, Loss: 0.0070\n",
      "Batch 10/23, Loss: 0.0044\n",
      "Batch 15/23, Loss: 0.0082\n",
      "Batch 20/23, Loss: 0.0057\n",
      "Epoch 54/100, Loss: 0.0076\n",
      "Batch 0/23, Loss: 0.0068\n",
      "Batch 5/23, Loss: 0.0052\n",
      "Batch 10/23, Loss: 0.0076\n",
      "Batch 15/23, Loss: 0.0067\n",
      "Batch 20/23, Loss: 0.0026\n",
      "Epoch 55/100, Loss: 0.0072\n",
      "Batch 0/23, Loss: 0.0107\n",
      "Batch 5/23, Loss: 0.0081\n",
      "Batch 10/23, Loss: 0.0110\n",
      "Batch 15/23, Loss: 0.0076\n",
      "Batch 20/23, Loss: 0.0124\n",
      "Epoch 56/100, Loss: 0.0075\n",
      "Batch 0/23, Loss: 0.0086\n",
      "Batch 5/23, Loss: 0.0036\n",
      "Batch 10/23, Loss: 0.0034\n",
      "Batch 15/23, Loss: 0.0033\n",
      "Batch 20/23, Loss: 0.0052\n",
      "Epoch 57/100, Loss: 0.0071\n",
      "Batch 0/23, Loss: 0.0061\n",
      "Batch 5/23, Loss: 0.0079\n",
      "Batch 10/23, Loss: 0.0038\n",
      "Batch 15/23, Loss: 0.0123\n",
      "Batch 20/23, Loss: 0.0164\n",
      "Epoch 58/100, Loss: 0.0065\n",
      "Batch 0/23, Loss: 0.0042\n",
      "Batch 5/23, Loss: 0.0016\n",
      "Batch 10/23, Loss: 0.0050\n",
      "Batch 15/23, Loss: 0.0074\n",
      "Batch 20/23, Loss: 0.0069\n",
      "Epoch 59/100, Loss: 0.0068\n",
      "Batch 0/23, Loss: 0.0017\n",
      "Batch 5/23, Loss: 0.0051\n",
      "Batch 10/23, Loss: 0.0115\n",
      "Batch 15/23, Loss: 0.0093\n",
      "Batch 20/23, Loss: 0.0092\n",
      "Epoch 60/100, Loss: 0.0065\n",
      "Batch 0/23, Loss: 0.0036\n",
      "Batch 5/23, Loss: 0.0036\n",
      "Batch 10/23, Loss: 0.0132\n",
      "Batch 15/23, Loss: 0.0056\n",
      "Batch 20/23, Loss: 0.0100\n",
      "Epoch 61/100, Loss: 0.0086\n",
      "Batch 0/23, Loss: 0.0026\n",
      "Batch 5/23, Loss: 0.0017\n",
      "Batch 10/23, Loss: 0.0033\n",
      "Batch 15/23, Loss: 0.0028\n",
      "Batch 20/23, Loss: 0.0104\n",
      "Epoch 62/100, Loss: 0.0071\n",
      "Batch 0/23, Loss: 0.0036\n",
      "Batch 5/23, Loss: 0.0096\n",
      "Batch 10/23, Loss: 0.0111\n",
      "Batch 15/23, Loss: 0.0032\n",
      "Batch 20/23, Loss: 0.0051\n",
      "Epoch 63/100, Loss: 0.0076\n",
      "Batch 0/23, Loss: 0.0061\n",
      "Batch 5/23, Loss: 0.0062\n",
      "Batch 10/23, Loss: 0.0089\n",
      "Batch 15/23, Loss: 0.0076\n",
      "Batch 20/23, Loss: 0.0042\n",
      "Epoch 64/100, Loss: 0.0072\n",
      "Batch 0/23, Loss: 0.0066\n",
      "Batch 5/23, Loss: 0.0055\n",
      "Batch 10/23, Loss: 0.0055\n",
      "Batch 15/23, Loss: 0.0079\n",
      "Batch 20/23, Loss: 0.0032\n",
      "Epoch 65/100, Loss: 0.0066\n",
      "Batch 0/23, Loss: 0.0042\n",
      "Batch 5/23, Loss: 0.0050\n",
      "Batch 10/23, Loss: 0.0104\n",
      "Batch 15/23, Loss: 0.0047\n",
      "Batch 20/23, Loss: 0.0031\n",
      "Epoch 66/100, Loss: 0.0063\n",
      "Batch 0/23, Loss: 0.0015\n",
      "Batch 5/23, Loss: 0.0025\n",
      "Batch 10/23, Loss: 0.0029\n",
      "Batch 15/23, Loss: 0.0011\n",
      "Batch 20/23, Loss: 0.0082\n",
      "Epoch 67/100, Loss: 0.0055\n",
      "Batch 0/23, Loss: 0.0060\n",
      "Batch 5/23, Loss: 0.0041\n",
      "Batch 10/23, Loss: 0.0022\n",
      "Batch 15/23, Loss: 0.0090\n",
      "Batch 20/23, Loss: 0.0044\n",
      "Epoch 68/100, Loss: 0.0065\n",
      "Batch 0/23, Loss: 0.0129\n",
      "Batch 5/23, Loss: 0.0036\n",
      "Batch 10/23, Loss: 0.0107\n",
      "Batch 15/23, Loss: 0.0075\n",
      "Batch 20/23, Loss: 0.0166\n",
      "Epoch 69/100, Loss: 0.0080\n",
      "Batch 0/23, Loss: 0.0068\n",
      "Batch 5/23, Loss: 0.0026\n",
      "Batch 10/23, Loss: 0.0077\n",
      "Batch 15/23, Loss: 0.0131\n",
      "Batch 20/23, Loss: 0.0024\n",
      "Epoch 70/100, Loss: 0.0057\n",
      "Batch 0/23, Loss: 0.0026\n",
      "Batch 5/23, Loss: 0.0078\n",
      "Batch 10/23, Loss: 0.0032\n",
      "Batch 15/23, Loss: 0.0022\n",
      "Batch 20/23, Loss: 0.0060\n",
      "Epoch 71/100, Loss: 0.0060\n",
      "Batch 0/23, Loss: 0.0093\n",
      "Batch 5/23, Loss: 0.0033\n",
      "Batch 10/23, Loss: 0.0043\n",
      "Batch 15/23, Loss: 0.0077\n",
      "Batch 20/23, Loss: 0.0073\n",
      "Epoch 72/100, Loss: 0.0052\n",
      "Batch 0/23, Loss: 0.0066\n",
      "Batch 5/23, Loss: 0.0025\n",
      "Batch 10/23, Loss: 0.0111\n",
      "Batch 15/23, Loss: 0.0048\n",
      "Batch 20/23, Loss: 0.0036\n",
      "Epoch 73/100, Loss: 0.0066\n",
      "Batch 0/23, Loss: 0.0063\n",
      "Batch 5/23, Loss: 0.0051\n",
      "Batch 10/23, Loss: 0.0086\n",
      "Batch 15/23, Loss: 0.0100\n",
      "Batch 20/23, Loss: 0.0105\n",
      "Epoch 74/100, Loss: 0.0068\n",
      "Batch 0/23, Loss: 0.0083\n",
      "Batch 5/23, Loss: 0.0017\n",
      "Batch 10/23, Loss: 0.0049\n",
      "Batch 15/23, Loss: 0.0060\n",
      "Batch 20/23, Loss: 0.0052\n",
      "Epoch 75/100, Loss: 0.0047\n",
      "Batch 0/23, Loss: 0.0043\n",
      "Batch 5/23, Loss: 0.0021\n",
      "Batch 10/23, Loss: 0.0039\n",
      "Batch 15/23, Loss: 0.0045\n",
      "Batch 20/23, Loss: 0.0061\n",
      "Epoch 76/100, Loss: 0.0047\n",
      "Batch 0/23, Loss: 0.0050\n",
      "Batch 5/23, Loss: 0.0090\n",
      "Batch 10/23, Loss: 0.0054\n",
      "Batch 15/23, Loss: 0.0077\n",
      "Batch 20/23, Loss: 0.0116\n",
      "Epoch 77/100, Loss: 0.0058\n",
      "Batch 0/23, Loss: 0.0074\n",
      "Batch 5/23, Loss: 0.0055\n",
      "Batch 10/23, Loss: 0.0026\n",
      "Batch 15/23, Loss: 0.0136\n",
      "Batch 20/23, Loss: 0.0043\n",
      "Epoch 78/100, Loss: 0.0063\n",
      "Batch 0/23, Loss: 0.0021\n",
      "Batch 5/23, Loss: 0.0032\n",
      "Batch 10/23, Loss: 0.0070\n",
      "Batch 15/23, Loss: 0.0045\n",
      "Batch 20/23, Loss: 0.0045\n",
      "Epoch 79/100, Loss: 0.0060\n",
      "Batch 0/23, Loss: 0.0051\n",
      "Batch 5/23, Loss: 0.0069\n",
      "Batch 10/23, Loss: 0.0058\n",
      "Batch 15/23, Loss: 0.0031\n",
      "Batch 20/23, Loss: 0.0037\n",
      "Epoch 80/100, Loss: 0.0062\n",
      "Batch 0/23, Loss: 0.0038\n",
      "Batch 5/23, Loss: 0.0032\n",
      "Batch 10/23, Loss: 0.0047\n",
      "Batch 15/23, Loss: 0.0031\n",
      "Batch 20/23, Loss: 0.0033\n",
      "Epoch 81/100, Loss: 0.0052\n",
      "Batch 0/23, Loss: 0.0018\n",
      "Batch 5/23, Loss: 0.0080\n",
      "Batch 10/23, Loss: 0.0063\n",
      "Batch 15/23, Loss: 0.0058\n",
      "Batch 20/23, Loss: 0.0041\n",
      "Epoch 82/100, Loss: 0.0051\n",
      "Batch 0/23, Loss: 0.0039\n",
      "Batch 5/23, Loss: 0.0076\n",
      "Batch 10/23, Loss: 0.0092\n",
      "Batch 15/23, Loss: 0.0056\n",
      "Batch 20/23, Loss: 0.0049\n",
      "Epoch 83/100, Loss: 0.0072\n",
      "Batch 0/23, Loss: 0.0031\n",
      "Batch 5/23, Loss: 0.0042\n",
      "Batch 10/23, Loss: 0.0074\n",
      "Batch 15/23, Loss: 0.0076\n",
      "Batch 20/23, Loss: 0.0026\n",
      "Epoch 84/100, Loss: 0.0048\n",
      "Batch 0/23, Loss: 0.0065\n",
      "Batch 5/23, Loss: 0.0034\n",
      "Batch 10/23, Loss: 0.0031\n",
      "Batch 15/23, Loss: 0.0049\n",
      "Batch 20/23, Loss: 0.0104\n",
      "Epoch 85/100, Loss: 0.0052\n",
      "Batch 0/23, Loss: 0.0046\n",
      "Batch 5/23, Loss: 0.0037\n",
      "Batch 10/23, Loss: 0.0032\n",
      "Batch 15/23, Loss: 0.0038\n",
      "Batch 20/23, Loss: 0.0064\n",
      "Epoch 86/100, Loss: 0.0049\n",
      "Batch 0/23, Loss: 0.0083\n",
      "Batch 5/23, Loss: 0.0041\n",
      "Batch 10/23, Loss: 0.0041\n",
      "Batch 15/23, Loss: 0.0046\n",
      "Batch 20/23, Loss: 0.0095\n",
      "Epoch 87/100, Loss: 0.0052\n",
      "Batch 0/23, Loss: 0.0031\n",
      "Batch 5/23, Loss: 0.0016\n",
      "Batch 10/23, Loss: 0.0034\n",
      "Batch 15/23, Loss: 0.0033\n",
      "Batch 20/23, Loss: 0.0034\n",
      "Epoch 88/100, Loss: 0.0043\n",
      "Batch 0/23, Loss: 0.0022\n",
      "Batch 5/23, Loss: 0.0010\n",
      "Batch 10/23, Loss: 0.0022\n",
      "Batch 15/23, Loss: 0.0047\n",
      "Batch 20/23, Loss: 0.0028\n",
      "Epoch 89/100, Loss: 0.0043\n",
      "Batch 0/23, Loss: 0.0045\n",
      "Batch 5/23, Loss: 0.0059\n",
      "Batch 10/23, Loss: 0.0018\n",
      "Batch 15/23, Loss: 0.0022\n",
      "Batch 20/23, Loss: 0.0058\n",
      "Epoch 90/100, Loss: 0.0046\n",
      "Batch 0/23, Loss: 0.0012\n",
      "Batch 5/23, Loss: 0.0037\n",
      "Batch 10/23, Loss: 0.0041\n",
      "Batch 15/23, Loss: 0.0180\n",
      "Batch 20/23, Loss: 0.0034\n",
      "Epoch 91/100, Loss: 0.0044\n",
      "Batch 0/23, Loss: 0.0018\n",
      "Batch 5/23, Loss: 0.0051\n",
      "Batch 10/23, Loss: 0.0048\n",
      "Batch 15/23, Loss: 0.0016\n",
      "Batch 20/23, Loss: 0.0054\n",
      "Epoch 92/100, Loss: 0.0044\n",
      "Batch 0/23, Loss: 0.0071\n",
      "Batch 5/23, Loss: 0.0051\n",
      "Batch 10/23, Loss: 0.0066\n",
      "Batch 15/23, Loss: 0.0012\n",
      "Batch 20/23, Loss: 0.0041\n",
      "Epoch 93/100, Loss: 0.0040\n",
      "Batch 0/23, Loss: 0.0028\n",
      "Batch 5/23, Loss: 0.0023\n",
      "Batch 10/23, Loss: 0.0025\n",
      "Batch 15/23, Loss: 0.0030\n",
      "Batch 20/23, Loss: 0.0038\n",
      "Epoch 94/100, Loss: 0.0036\n",
      "Batch 0/23, Loss: 0.0037\n",
      "Batch 5/23, Loss: 0.0045\n",
      "Batch 10/23, Loss: 0.0030\n",
      "Batch 15/23, Loss: 0.0049\n",
      "Batch 20/23, Loss: 0.0019\n",
      "Epoch 95/100, Loss: 0.0041\n",
      "Batch 0/23, Loss: 0.0070\n",
      "Batch 5/23, Loss: 0.0066\n",
      "Batch 10/23, Loss: 0.0068\n",
      "Batch 15/23, Loss: 0.0038\n",
      "Batch 20/23, Loss: 0.0073\n",
      "Epoch 96/100, Loss: 0.0049\n",
      "Batch 0/23, Loss: 0.0017\n",
      "Batch 5/23, Loss: 0.0024\n",
      "Batch 10/23, Loss: 0.0046\n",
      "Batch 15/23, Loss: 0.0015\n",
      "Batch 20/23, Loss: 0.0067\n",
      "Epoch 97/100, Loss: 0.0042\n",
      "Batch 0/23, Loss: 0.0023\n",
      "Batch 5/23, Loss: 0.0058\n",
      "Batch 10/23, Loss: 0.0020\n",
      "Batch 15/23, Loss: 0.0053\n",
      "Batch 20/23, Loss: 0.0029\n",
      "Epoch 98/100, Loss: 0.0045\n",
      "Batch 0/23, Loss: 0.0023\n",
      "Batch 5/23, Loss: 0.0045\n",
      "Batch 10/23, Loss: 0.0120\n",
      "Batch 15/23, Loss: 0.0028\n",
      "Batch 20/23, Loss: 0.0032\n",
      "Epoch 99/100, Loss: 0.0043\n",
      "Batch 0/23, Loss: 0.0022\n",
      "Batch 5/23, Loss: 0.0016\n",
      "Batch 10/23, Loss: 0.0030\n",
      "Batch 15/23, Loss: 0.0047\n",
      "Batch 20/23, Loss: 0.0046\n",
      "Epoch 100/100, Loss: 0.0043\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_epoch(model, feedback_dataloader, optimizer, criterion, device)\n",
    "\n",
    "    scheduler.step(train_loss)\n",
    "\n",
    "    if train_loss < best_loss:\n",
    "        best_loss = train_loss\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': best_loss,\n",
    "        }, 'best_siamese_model.pt')\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {train_loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TagLBqsEIqaD"
   },
   "source": [
    "### Update Embeddings ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "executionInfo": {
     "elapsed": 856,
     "status": "ok",
     "timestamp": 1733172582827,
     "user": {
      "displayName": "Neil Auroni",
      "userId": "12234025139407697306"
     },
     "user_tz": 300
    },
    "id": "qUrJcDHOIcgB"
   },
   "outputs": [],
   "source": [
    "def update_embeddings(embeddings, file_names, siamese_model, device, batch_size=32):\n",
    "    siamese_model.eval()\n",
    "    updated_embeddings = []\n",
    "\n",
    "    for i in range(0, len(embeddings), batch_size):\n",
    "        batch = embeddings[i:i + batch_size]\n",
    "\n",
    "        batch_tensor = torch.tensor(batch, dtype=torch.float32).to(device)\n",
    "\n",
    "        if len(batch_tensor.shape) == 2:\n",
    "            batch_tensor = batch_tensor.unsqueeze(0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            updated_batch = siamese_model.forward_one(batch_tensor)\n",
    "            updated_embeddings.extend(updated_batch.cpu().numpy())\n",
    "\n",
    "    return np.array(updated_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "executionInfo": {
     "elapsed": 888,
     "status": "ok",
     "timestamp": 1733172587943,
     "user": {
      "displayName": "Neil Auroni",
      "userId": "12234025139407697306"
     },
     "user_tz": 300
    },
    "id": "Xv4Fd9ZNItUG"
   },
   "outputs": [],
   "source": [
    "def cosine_similarity(a, b):\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1733172610350,
     "user": {
      "displayName": "Neil Auroni",
      "userId": "12234025139407697306"
     },
     "user_tz": 300
    },
    "id": "RSLEluZbIukJ"
   },
   "outputs": [],
   "source": [
    "def get_similar_songs(track_id, embeddings_dict, siamese_model, device, top_n=10):\n",
    "    siamese_model.eval()\n",
    "\n",
    "    query_song = track_id + '.wav'\n",
    "\n",
    "    query_embedding = embeddings_dict[track_id]\n",
    "    query_tensor = torch.tensor(query_embedding, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "\n",
    "    similarities = []\n",
    "\n",
    "    for song_id in embeddings_dict.keys():\n",
    "        if song_id + '.wav' == query_song:\n",
    "            continue\n",
    "\n",
    "        comp_embedding = embeddings_dict[song_id]\n",
    "        comp_tensor = torch.tensor(comp_embedding, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            query_updated = siamese_model.forward_one(query_tensor)\n",
    "            comp_updated = siamese_model.forward_one(comp_tensor)\n",
    "\n",
    "            query_updated_np = query_updated.cpu().numpy()\n",
    "            comp_updated_np = comp_updated.cpu().numpy()\n",
    "\n",
    "            similarity = cosine_similarity(query_updated_np.flatten(), comp_updated_np.flatten())\n",
    "            similarities.append((song_id, similarity))\n",
    "\n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    print(f\"\\nUpdated similar songs to {track_id}:\")\n",
    "    for song_id, score in similarities[:top_n]:\n",
    "        print(f\"{song_id}: Similarity Score = {score:.3f}\")\n",
    "\n",
    "    return similarities[:top_n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dPC2dYkTI10c"
   },
   "source": [
    "### Recommendation ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "executionInfo": {
     "elapsed": 856,
     "status": "ok",
     "timestamp": 1733172673321,
     "user": {
      "displayName": "Neil Auroni",
      "userId": "12234025139407697306"
     },
     "user_tz": 300
    },
    "id": "sBaYAOJ7JCyH"
   },
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "executionInfo": {
     "elapsed": 964,
     "status": "ok",
     "timestamp": 1733172681233,
     "user": {
      "displayName": "Neil Auroni",
      "userId": "12234025139407697306"
     },
     "user_tz": 300
    },
    "id": "wkTFLfNlJDhQ"
   },
   "outputs": [],
   "source": [
    "def load_embeddings(embeddings_dir):\n",
    "    embeddings_dict = {}\n",
    "    embedding_files = glob.glob(os.path.join(embeddings_dir, '*_embedding.npy'))\n",
    "\n",
    "    print(f\"Loading {len(embedding_files)} embedding files...\")\n",
    "\n",
    "    for file_path in embedding_files:\n",
    "        try:\n",
    "            embedding = np.load(file_path)\n",
    "\n",
    "            file_name = os.path.basename(file_path)\n",
    "            track_id = file_name.replace('_embedding.npy', '')\n",
    "            embeddings_dict[track_id] = embedding\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {file_path}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    print(f\"Successfully loaded {len(embeddings_dict)} embeddings\")\n",
    "    return embeddings_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 234,
     "status": "ok",
     "timestamp": 1733173740266,
     "user": {
      "displayName": "Neil Auroni",
      "userId": "12234025139407697306"
     },
     "user_tz": 300
    },
    "id": "oVy_aOHcIywc",
    "outputId": "2508dd0c-3cf9-4a3e-96a5-2bdaac10b81b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 30 embedding files...\n",
      "Successfully loaded 30 embeddings\n"
     ]
    }
   ],
   "source": [
    "embeddings_dict = load_embeddings(EMBEDDINGS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1733173742159,
     "user": {
      "displayName": "Neil Auroni",
      "userId": "12234025139407697306"
     },
     "user_tz": 300
    },
    "id": "urgJt5PnI3mn",
    "outputId": "29886bcf-0604-4010-9988-18ace58c27f8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-68-93d89efc67e4>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(model_path)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SiameseNetwork(\n",
       "  (frame_encoder): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.2, inplace=False)\n",
       "    (3): Linear(in_features=256, out_features=128, bias=True)\n",
       "    (4): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (transformer): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-2): 3 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "        (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.2, inplace=False)\n",
       "        (dropout2): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (similarity_net): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.2, inplace=False)\n",
       "    (3): Linear(in_features=64, out_features=32, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): Dropout(p=0.2, inplace=False)\n",
       "    (6): Linear(in_features=32, out_features=1, bias=True)\n",
       "    (7): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = '/best_siamese_model.pt'\n",
    "checkpoint = torch.load(model_path)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "replace '...wav' with file name of song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 242,
     "status": "ok",
     "timestamp": 1733173744181,
     "user": {
      "displayName": "Neil Auroni",
      "userId": "12234025139407697306"
     },
     "user_tz": 300
    },
    "id": "BXxEZ4o9JNCf",
    "outputId": "e2717a10-2c82-4ffe-b66b-da20d0c71e31"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Updated similar songs to Drake - Hotline Bling_snippet.wav:\n",
      "Future - WAIT FOR U (feat. Drake & Tems)_snippet.wav: Similarity Score = 0.459\n",
      "Kendrick Lamar - luther_snippet.wav: Similarity Score = 0.438\n",
      "Mac Miller - My Favorite Part_snippet.wav: Similarity Score = 0.415\n",
      "Drake - Too Good_snippet.wav: Similarity Score = 0.389\n",
      "Drake - One Dance_snippet.wav: Similarity Score = 0.378\n",
      "Frank Ocean - In My Room_snippet.wav: Similarity Score = 0.308\n",
      "Drake - Controlla_snippet.wav: Similarity Score = 0.308\n",
      "French Montana - Unforgettable (feat. Swae Lee)_snippet.wav: Similarity Score = 0.305\n",
      "Mac Miller - Wings_snippet.wav: Similarity Score = 0.294\n",
      "Mac Miller - Knock Knock_snippet.wav: Similarity Score = 0.256\n"
     ]
    }
   ],
   "source": [
    "similar_songs = get_similar_songs(\n",
    "    \"....wav\",\n",
    "    embeddings_dict,\n",
    "    model,\n",
    "    device,\n",
    "    top_n=10\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOucsr4ajGgcuf4Tup88GDo",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
